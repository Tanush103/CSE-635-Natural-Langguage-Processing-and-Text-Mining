{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BasicBERT_Task3_BERT_Large_Uncased.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NznKwIEh05iQ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pprint import  pprint\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "6W7-WRaG1AkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task3\n",
        "train_data=pd.read_csv('train.tsv',sep='\\t')\n",
        "valid_data=pd.read_csv('valid.tsv',sep='\\t')\n",
        "\n",
        "train_data['label'].replace({\"Lit-News_mentions\":0, \"Nonpersonal_reports\":1, \"Self_reports\":2}, inplace=True)\n",
        "valid_data['label'].replace({\"Lit-News_mentions\":0, \"Nonpersonal_reports\":1, \"Self_reports\":2}, inplace=True)"
      ],
      "metadata": {
        "id": "-3MfkUMnZpQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "UL-Nzw6u4Tsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\",do_lower_case=True)\n"
      ],
      "metadata": {
        "id": "LkAf2QNm1Ap0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc = tokenizer.batch_encode_plus(train_data['tweet'].tolist(), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "test_enc = tokenizer.batch_encode_plus(valid_data['tweet'].tolist(), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "zulqP4pH1EwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LQZbOcL79y9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc.keys()"
      ],
      "metadata": {
        "id": "BzZGtdln1EzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(train_enc[\"input_ids\"][3])"
      ],
      "metadata": {
        "id": "A5XNcS1J96m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(train_enc[\"input_ids\"][-3])"
      ],
      "metadata": {
        "id": "AsjJkM6_-hAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc[\"attention_mask\"]"
      ],
      "metadata": {
        "id": "2MTcPYmX-by3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc[\"token_type_ids\"]"
      ],
      "metadata": {
        "id": "gQetqO7F-NqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc[\"input_ids\"]"
      ],
      "metadata": {
        "id": "ZeX_laIz915Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_type_ids, train_attn_mask = train_enc.input_ids, train_enc.token_type_ids, train_enc.attention_mask\n",
        "test_input_ids, test_type_ids, test_attn_mask = test_enc.input_ids, test_enc.token_type_ids, test_enc.attention_mask"
      ],
      "metadata": {
        "id": "E7Tj4ldn1E2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids.shape, train_type_ids.shape, train_attn_mask.shape"
      ],
      "metadata": {
        "id": "dMlFBbo41I7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Below we define a function to create train, test & valid dataloaders in Pytorch\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def get_dataloader(input_ids, type_ids, attn_mask, y):\n",
        "    data = TensorDataset(input_ids, type_ids, attn_mask, y)\n",
        "    sampler = RandomSampler(data)\n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_datalaoder = get_dataloader(train_input_ids, train_type_ids, train_attn_mask, torch.tensor(train_data['label']))\n",
        "test_datalaoder = get_dataloader(test_input_ids, test_type_ids, test_attn_mask, torch.tensor(valid_data['label']))"
      ],
      "metadata": {
        "id": "s0-MFobu1I9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check that the tensors returned by the dataloader are correct\n",
        "\n",
        "for batch in train_datalaoder:\n",
        "    input_ids, type_ids, attn_mask, y = batch\n",
        "    print(input_ids.shape, type_ids.shape, attn_mask.shape, y.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "Gvz3kCPq1JAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "-NJnaeQ_-1Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self, transformer):\n",
        "    super().__init__()\n",
        "    shape=1024\n",
        "    self.transformer = transformer\n",
        "    self.linear = nn.Linear(shape, 4)\n",
        "  \n",
        "  def forward(self, in_ids, type_ids, attn_mask):\n",
        "    op = self.transformer(input_ids=in_ids, attention_mask=attn_mask, \n",
        "                          token_type_ids=type_ids)\n",
        "    \n",
        "    return  self.linear(op[\"pooler_output\"])"
      ],
      "metadata": {
        "id": "wuE1zRt21JCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ],
      "metadata": {
        "id": "15skZnK31JEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#model = BERTClassifier(transformer).to(device)\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "\n",
        "transformer = AutoModel.from_pretrained(\"bert-large-uncased\")\n",
        "\n",
        "\n",
        "model = BERTClassifier(transformer).to(device)\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "sXj37Br41JGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "ewQcw4-T_bRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  if \"pooler\" in name or \"linear\" in name:#or \"layer.11\" in name or \"layer.10\" in name or \"linear\" in name:\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "  print(name, param.shape, param.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "1ukAkqry1RVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "for ix, batch in tqdm(enumerate(train_datalaoder)):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, type_ids, attn_mask, y = batch\n",
        "print(input_ids.shape)\n",
        "print(type_ids)\n",
        "print(attn_mask.shape)"
      ],
      "metadata": {
        "id": "JYOt4p4fkeez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function: Performs forward propagation, backpropagation & optimization.\n",
        "# We also implement gradient clipping, which prevents the gradients from exploding\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, clip=1.0):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    ep_t_loss = 0\n",
        "    batch_num  = 0\n",
        "    pred, tgt = [], []\n",
        "    for ix, batch in tqdm(enumerate(dataloader)):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, type_ids, attn_mask, y = batch\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_ids, type_ids, attn_mask)     \n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "\n",
        "        #gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        ep_t_loss += loss.item()\n",
        "        batch_num += 1\n",
        "        pred.extend(torch.argmax(output, -1).tolist())\n",
        "        tgt.extend(y.tolist())\n",
        "\n",
        "    return ep_t_loss/batch_num, metrics.f1_score(tgt, pred, average='macro')\n",
        "\n",
        "# Evaluation function: Calculates loss on the validation data.\n",
        "from sklearn import metrics\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    ep_t_loss = 0\n",
        "    batch_num  = 0\n",
        "    pred, tgt = [], []\n",
        "    for ix, batch in enumerate(dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, type_ids, attn_mask, y = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_ids, type_ids, attn_mask)\n",
        "            \n",
        "            loss = criterion(output, y)\n",
        "\n",
        "            ep_t_loss += loss.item()\n",
        "            batch_num += 1\n",
        "            pred.extend(torch.argmax(output, -1).tolist())\n",
        "            tgt.extend(y.tolist())\n",
        "        \n",
        "    return ep_t_loss/batch_num, metrics.f1_score(tgt, pred, average='macro'), pred, tgt"
      ],
      "metadata": {
        "id": "XwKfGeCy1ZMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optim = torch.optim.AdamW(model.parameters(), lr = 2e-5)"
      ],
      "metadata": {
        "id": "jri7AxPd1RYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "tot_t_loss, tot_v_loss =[],[]\n",
        "N_EPOCHS = 12"
      ],
      "metadata": {
        "id": "u8O5WEYd1RbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in tqdm(range(N_EPOCHS)): \n",
        "\n",
        "    tr_l, tr_f1= train(model, train_datalaoder, optim, criterion)\n",
        "    tot_t_loss.append(tr_l)\n",
        "\n",
        "    val_l, val_f1, pred, tgt = evaluate(model, test_datalaoder, criterion)\n",
        "    tot_v_loss.append(val_l)\n",
        "    \n",
        "    if val_l < best_valid_loss:\n",
        "        best_valid_loss = val_l\n",
        "        best_pred, best_tgt = pred, tgt\n",
        "        torch.save(model.state_dict(), 'model_least_loss.pt')\n",
        "        print(\"\\nBest Model Saved !!\")\n",
        "    elif epoch % 3 == 0:\n",
        "        torch.save(model.state_dict(), 'model_checkpoint_'+str(epoch)+'.pt')\n",
        "        print(\"\\Checkpoint Model Saved !!\")\n",
        "    print(\"\\n\")\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Total Loss: {tr_l:.3f} | Train F1: {tr_f1:.3f}')\n",
        "    print(f'\\tVal. Total Loss: {val_l:.3f} | Valid F1: {val_f1:.3f}')\n",
        "    print(\"_________________________________________________________________\")"
      ],
      "metadata": {
        "id": "pG9964yY1ReE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(best_tgt, best_pred))"
      ],
      "metadata": {
        "id": "BPPDiv1d1RhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert-Based-Uncased 12 Epochs - 0.95\n",
        "\n",
        "Bert-Large-Uncased 12 Epochs - 0.94 \n",
        "\n",
        "bertweet-covid19-base-uncased 12 epochs - 0.96\n",
        "\n",
        "digitalepidemiologylab/covid-twitter-bert 12 epochs - 0.97\n",
        "\n",
        "COVID-SciBERT 12 epochs - 0.97\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C6Lq8-eBCg3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q31wRnjo1RkQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}