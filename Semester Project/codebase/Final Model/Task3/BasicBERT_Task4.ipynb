{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BasicBERT_Task4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e9DFIF-OYxh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel,RobertaTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pprint import  pprint\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "9fGztBwsOjSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task4\n",
        "train_data=pd.read_csv('train.tsv',sep='\\t',header=None)\n",
        "valid_data=pd.read_csv('valid.tsv',sep='\\t')\n",
        "train_data.columns=['tweet_id','user_id','tweet','label']"
      ],
      "metadata": {
        "id": "n7trHmZNOjV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "so8iuVvuOjY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "def cleanText(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
        "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
        "    text = text.lower()\n",
        "    text = text.replace('x', '')\n",
        "    return text\n",
        "train_data['tweet'] = train_data['tweet'].apply(cleanText)\n",
        "valid_data['tweet'] = valid_data['tweet'].apply(cleanText)"
      ],
      "metadata": {
        "id": "MTZbc9bBhBLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer - Make changes as per readme file"
      ],
      "metadata": {
        "id": "OuCJXjdEg5Ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n",
        "#tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\",do_lower_case=True)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert\",do_lower_case=True)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"lordtt13/COVID-SciBERT\")\n"
      ],
      "metadata": {
        "id": "6-46DwfHOjcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc = tokenizer.batch_encode_plus(train_data['tweet'].tolist(), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "test_enc = tokenizer.batch_encode_plus(valid_data['tweet'].tolist(), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "NRKWc3hyOjfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc.keys()"
      ],
      "metadata": {
        "id": "UK2IHNKMOjiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(train_enc[\"input_ids\"][3])"
      ],
      "metadata": {
        "id": "oP-YdxAJOjle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(train_enc[\"input_ids\"][-3])"
      ],
      "metadata": {
        "id": "mqqXl3rkOjoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc[\"attention_mask\"]"
      ],
      "metadata": {
        "id": "B_WEk3wuOjry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc[\"token_type_ids\"]"
      ],
      "metadata": {
        "id": "GXES9D_JOjvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc[\"input_ids\"]"
      ],
      "metadata": {
        "id": "zuHNDGJWOjyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_type_ids, train_attn_mask = train_enc.input_ids, train_enc.token_type_ids, train_enc.attention_mask\n",
        "test_input_ids, test_type_ids, test_attn_mask = test_enc.input_ids, test_enc.token_type_ids, test_enc.attention_mask"
      ],
      "metadata": {
        "id": "Rg_n6fmGOj1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids.shape, train_type_ids.shape, train_attn_mask.shape"
      ],
      "metadata": {
        "id": "1PHbG316Oj4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Below we define a function to create train, test & valid dataloaders in Pytorch\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "def get_dataloader(input_ids, type_ids, attn_mask, y):\n",
        "    data = TensorDataset(input_ids, type_ids, attn_mask, y)\n",
        "    sampler = RandomSampler(data)\n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n",
        "    return dataloader\n",
        "\n",
        "train_datalaoder = get_dataloader(train_input_ids, train_type_ids, train_attn_mask, torch.tensor(train_data['label']))\n",
        "test_datalaoder = get_dataloader(test_input_ids, test_type_ids, test_attn_mask, torch.tensor(valid_data['label']))"
      ],
      "metadata": {
        "id": "RsvX7ZQ-Oj7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check that the tensors returned by the dataloader are correct\n",
        "\n",
        "for batch in train_datalaoder:\n",
        "    input_ids, type_ids, attn_mask, y = batch\n",
        "    print(input_ids.shape, type_ids.shape, attn_mask.shape, y.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "7m6KZXplOj-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "QFyQfT0bOkCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "change the shape variable as per the readme file "
      ],
      "metadata": {
        "id": "NRM_8YCPiB7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self, transformer):\n",
        "    super().__init__()\n",
        "    shape=768\n",
        "    self.transformer = transformer\n",
        "    self.linear = nn.Linear(shape, 4)\n",
        "  \n",
        "  def forward(self, in_ids, type_ids, attn_mask):\n",
        "    op = self.transformer(input_ids=in_ids, attention_mask=attn_mask, \n",
        "                          token_type_ids=type_ids)\n",
        "    \n",
        "    return  self.linear(op[\"pooler_output\"])"
      ],
      "metadata": {
        "id": "nvk6a4UUPCTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ],
      "metadata": {
        "id": "Ioq4v2UUPCWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model- Make Changes as per readme file "
      ],
      "metadata": {
        "id": "C5H6Ef8Hh9-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#model = BERTClassifier(transformer).to(device)\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "#transformer = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "#transformer = AutoModel.from_pretrained(\"bert-large-uncased\")\n",
        "#transformer = AutoModel.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert\")\n",
        "#transformer = AutoModel.from_pretrained(\"lordtt13/COVID-SciBERT\")\n",
        "\n",
        "model = BERTClassifier(transformer).to(device)\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "vmzE4icHPCZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "KB72bK9BPCc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  if \"pooler\" in name or \"linear\" in name:#or \"layer.11\" in name or \"layer.10\" in name or \"linear\" in name:\n",
        "    param.requires_grad = True\n",
        "  else:\n",
        "    param.requires_grad = False\n",
        "  print(name, param.shape, param.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "enKvuhGlPCgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "for ix, batch in tqdm(enumerate(train_datalaoder)):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, type_ids, attn_mask, y = batch\n",
        "print(input_ids.shape)\n",
        "print(type_ids)\n",
        "print(attn_mask.shape)"
      ],
      "metadata": {
        "id": "8yKmWHvVPCkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function: Performs forward propagation, backpropagation & optimization.\n",
        "# We also implement gradient clipping, which prevents the gradients from exploding\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, clip=1.0):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    ep_t_loss = 0\n",
        "    batch_num  = 0\n",
        "    pred, tgt = [], []\n",
        "    for ix, batch in tqdm(enumerate(dataloader)):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, type_ids, attn_mask, y = batch\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_ids, type_ids, attn_mask)     \n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "\n",
        "        #gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        ep_t_loss += loss.item()\n",
        "        batch_num += 1\n",
        "        pred.extend(torch.argmax(output, -1).tolist())\n",
        "        tgt.extend(y.tolist())\n",
        "\n",
        "    return ep_t_loss/batch_num, metrics.f1_score(tgt, pred, average='macro')\n",
        "\n",
        "# Evaluation function: Calculates loss on the validation data.\n",
        "from sklearn import metrics\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    ep_t_loss = 0\n",
        "    batch_num  = 0\n",
        "    pred, tgt = [], []\n",
        "    for ix, batch in enumerate(dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, type_ids, attn_mask, y = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_ids, type_ids, attn_mask)\n",
        "            \n",
        "            loss = criterion(output, y)\n",
        "\n",
        "            ep_t_loss += loss.item()\n",
        "            batch_num += 1\n",
        "            pred.extend(torch.argmax(output, -1).tolist())\n",
        "            tgt.extend(y.tolist())\n",
        "        \n",
        "    return ep_t_loss/batch_num, metrics.f1_score(tgt, pred, average='macro'), pred, tgt"
      ],
      "metadata": {
        "id": "nyBTlulvPCns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optim = torch.optim.AdamW(model.parameters(), lr = 2e-5)"
      ],
      "metadata": {
        "id": "dw9oLcCUPCq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "tot_t_loss, tot_v_loss =[],[]\n",
        "N_EPOCHS = 12"
      ],
      "metadata": {
        "id": "HoTunUZlPCug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in tqdm(range(N_EPOCHS)): \n",
        "\n",
        "    tr_l, tr_f1= train(model, train_datalaoder, optim, criterion)\n",
        "    tot_t_loss.append(tr_l)\n",
        "\n",
        "    val_l, val_f1, pred, tgt = evaluate(model, test_datalaoder, criterion)\n",
        "    tot_v_loss.append(val_l)\n",
        "    \n",
        "    if val_l < best_valid_loss:\n",
        "        best_valid_loss = val_l\n",
        "        best_pred, best_tgt = pred, tgt\n",
        "        torch.save(model.state_dict(), 'model_least_loss.pt')\n",
        "        print(\"\\nBest Model Saved !!\")\n",
        "    elif epoch % 3 == 0:\n",
        "        torch.save(model.state_dict(), 'model_checkpoint_'+str(epoch)+'.pt')\n",
        "        print(\"\\Checkpoint Model Saved !!\")\n",
        "    print(\"\\n\")\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Total Loss: {tr_l:.3f} | Train F1: {tr_f1:.3f}')\n",
        "    print(f'\\tVal. Total Loss: {val_l:.3f} | Valid F1: {val_f1:.3f}')\n",
        "    print(\"_________________________________________________________________\")"
      ],
      "metadata": {
        "id": "vHzwJgSXPCzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(best_tgt, best_pred))"
      ],
      "metadata": {
        "id": "EheAXi9LPYZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.f1_score(best_tgt, best_pred))"
      ],
      "metadata": {
        "id": "X6Ztb8CMUOr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert-Based-Uncased 12 Epochs - 0.28\n",
        "\n",
        "Bert-Large-Uncased 12 Epochs - 0.18 \n",
        "\n",
        "\n",
        "COVID-SciBERT 12 epochs - 0.34\n",
        "\n",
        "digitalepidemiologylab/covid-twitter-bert 12 epochs - 0.53"
      ],
      "metadata": {
        "id": "9gNsY-c-Vk8a"
      }
    }
  ]
}