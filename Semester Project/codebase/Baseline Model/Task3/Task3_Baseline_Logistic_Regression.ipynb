{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task3 Baseline- Logistic Regression",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RvtqhluyxgF4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task3\n",
        "train_data=pd.read_csv('train.tsv',sep='\\t')\n",
        "valid_data=pd.read_csv('valid.tsv',sep='\\t')\n",
        "\n",
        "train_data['label'].replace({\"Lit-News_mentions\":0, \"Nonpersonal_reports\":1, \"Self_reports\":2}, inplace=True)\n",
        "valid_data['label'].replace({\"Lit-News_mentions\":0, \"Nonpersonal_reports\":1, \"Self_reports\":2}, inplace=True)\n",
        "\n",
        "X_train=train_data['tweet']\n",
        "y_train=train_data['label']\n",
        "X_test=valid_data['tweet']\n",
        "y_test=valid_data['label']"
      ],
      "metadata": {
        "id": "FqbpH8dGx99F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "doc2vec with logistic regression"
      ],
      "metadata": {
        "id": "-igXnx3YyqoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "def cleanText(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
        "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
        "    text = text.lower()\n",
        "    text = text.replace('x', '')\n",
        "    return text\n",
        "train_data['tweet'] = train_data['tweet'].apply(cleanText)\n",
        "valid_data['tweet'] = valid_data['tweet'].apply(cleanText)\n"
      ],
      "metadata": {
        "id": "QjKtJmHhx6RG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "def tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word.lower())\n",
        "    return tokens\n",
        "#train_tagged=train_data['tweet'].apply(tokenize_text)\n",
        "#valid_tagged=valid_data['tweet'].apply(tokenize_text)\n",
        "\n",
        "train_tagged = train_data.apply(\n",
        "    lambda r: TaggedDocument(words=tokenize_text(r['tweet']), tags=[r.label]), axis=1)\n",
        "test_tagged = valid_data.apply(\n",
        "    lambda r: TaggedDocument(words=tokenize_text(r['tweet']), tags=[r.label]), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TEr_ApoyJG0",
        "outputId": "6b9f392f-e637-4340-dd09-ac2ed7ad7c1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()"
      ],
      "metadata": {
        "id": "_aecxGMEyJQf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models import Doc2Vec\n",
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
        "model_dbow.build_vocab([x for x in tqdm(train_tagged)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvo19YTgyJZX",
        "outputId": "af288033-78dc-4595-821d-8075b19c5da9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1562695.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tagged.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqsc6LsTyQsg",
        "outputId": "e85d2715-93eb-42aa-bca0-09617a8b8714"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([TaggedDocument(words=['growing', 'number', 'of', 'covid-19', 'patients', 'whose', 'symptoms', 'were', 'initially', 'mild', 'are', 'now', 'facing', 'mysterious', 'long-term', 'neurological', 'problems', 'url'], tags=[0]),\n",
              "       TaggedDocument(words=['medical', 'eperts', 'advise', 'that', 'symptoms', 'of', 'the', 'novel', 'coronavirus', 'include', 'fever', 'shortness', 'of', 'breath', 'and', 'stinky', 'smelly', 'pits', 'and', 'feet', 'ðŸ˜³ðŸ¤ª'], tags=[0]),\n",
              "       TaggedDocument(words=['drdavidsamadi', 'hubby/i', 'same', 'symptoms', 'november', '2019', 'after', 'weekend', 'trip', 'vegas', 'where', 'bus', 'loads', 'of', 'chinese', 'tourists.1', 'day', 'fever,3', 'days', 'sore', 'throat', 'several', 'weeks', 'of', 'fatigue.he', \"'s\", 'healthy', \"'m\", 'not', 'diabetes', 'hypertension', 'obese', 'respiratory', 'issues', '53.', 'no', 'meds/pneumonia', 'we', 'believe', 'was', 'covid-19'], tags=[1]),\n",
              "       ...,\n",
              "       TaggedDocument(words=['yes', 'but', 'didn', 'recovered', 'with', 'tylenol', 'cough', 'medicine', 'added', 'zinc', 'after', 'my', 'sister', 'niece', 'ended', 'up', 'in', 'hospital', 'for', 'this', 'and', 'given', 'zinc', 'steroids', 'almost', 'immediately', 'the', 'one', 'with', 'prior', 'respiratory', 'problems', 'didn', 'survive'], tags=[1]),\n",
              "       TaggedDocument(words=['hello', 'guys', 'just', 'want', 'to', 'put', 'it', 'out', 'there', 'that', 'please', 'ignore', 'whatsapp', 'messages', 'for', 'covid-19', 'and', 'that', 'it', 'is', 'not', 'serious', 'disease', 'and', 'all', 'lost', 'my', 'father', 'to', 'covid-19', 'last', 'month', 'he', 'was', 'healthy', 'person', 'with', 'no', 'complications', 'of', 'diabetes', 'or', 'bp.he', 'developed', 'fever', 'and', 'the', 'fever'], tags=[1]),\n",
              "       TaggedDocument(words=['``', 'she', 'then', 'again', 'tried', 'to', 'end', 'the', 'call', 'and', 'wanted', 'to', 'schedule', 'the', 'net', 'one', 'the', 'call', 'had', 'only', 'been', 'few', 'minutes', 'so', 'far', 'asked', 'her', 'about', 'my', 'lower', 'body', 'symptoms', 'given', 'the', 'legs', 'mri', 'had', 'shown', 'no', 'signs', 'of', 'damage', 'she', 'just', 'said', 'it', 'was', '``', \"''\", 'post', 'viral', \"''\", \"''\", 'and', 'nothing', 'else', '10/n'], tags=[2])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn import utils\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_02qrkryTTH",
        "outputId": "1b9330f4-d149-40d8-d92d-63442df8ddde"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1218785.19it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1612455.14it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2801455.20it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2642606.79it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2496865.23it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2735559.95it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1682881.42it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2776502.47it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2756378.51it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2559547.34it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1402432.21it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2727124.73it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1367140.75it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2720102.59it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1424175.35it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2642239.59it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1904725.75it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1437852.26it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2310153.95it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1027858.98it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1628404.31it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2733004.27it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2630724.57it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1470885.88it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1239844.63it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1543916.63it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2736938.06it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1464993.04it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1917305.49it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1911138.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 32.1 s, sys: 2.37 s, total: 34.4 s\n",
            "Wall time: 20 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, regressors"
      ],
      "metadata": {
        "id": "U2CM0JFQyZHt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
        "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
        "logreg = LogisticRegression(n_jobs=1, C=1e5,max_iter=10000)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred,average='micro')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brQ3iJUWyd5t",
        "outputId": "6b3f0ae4-ff13-4645-c9b6-d787b36570fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy 0.616\n",
            "Testing F1 score: 0.616\n"
          ]
        }
      ]
    }
  ]
}