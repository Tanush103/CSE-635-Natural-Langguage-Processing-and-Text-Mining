{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task3 Baseline- Support Vector Classifier",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nweMLpI2xcOV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task3\n",
        "train_data=pd.read_csv('train.tsv',sep='\\t')\n",
        "valid_data=pd.read_csv('valid.tsv',sep='\\t')\n",
        "\n",
        "train_data['label'].replace({\"Lit-News_mentions\":0, \"Nonpersonal_reports\":1, \"Self_reports\":2}, inplace=True)\n",
        "valid_data['label'].replace({\"Lit-News_mentions\":0, \"Nonpersonal_reports\":1, \"Self_reports\":2}, inplace=True)\n",
        "\n",
        "X_train=train_data['tweet']\n",
        "y_train=train_data['label']\n",
        "X_test=valid_data['tweet']\n",
        "y_test=valid_data['label']"
      ],
      "metadata": {
        "id": "XD8INFQ9yjb-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "doc2vec with svm"
      ],
      "metadata": {
        "id": "z9oPQWLRyvNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "def cleanText(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
        "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
        "    text = text.lower()\n",
        "    text = text.replace('x', '')\n",
        "    return text\n",
        "train_data['tweet'] = train_data['tweet'].apply(cleanText)\n",
        "valid_data['tweet'] = valid_data['tweet'].apply(cleanText)\n"
      ],
      "metadata": {
        "id": "8UErt-F2ywXo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "def tokenize_text(text):\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word.lower())\n",
        "    return tokens\n",
        "#train_tagged=train_data['tweet'].apply(tokenize_text)\n",
        "#valid_tagged=valid_data['tweet'].apply(tokenize_text)\n",
        "\n",
        "train_tagged = train_data.apply(\n",
        "    lambda r: TaggedDocument(words=tokenize_text(r['tweet']), tags=[r.label]), axis=1)\n",
        "test_tagged = valid_data.apply(\n",
        "    lambda r: TaggedDocument(words=tokenize_text(r['tweet']), tags=[r.label]), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TKChh79ywag",
        "outputId": "d554e11b-8b7f-4a52-945b-a20530f2fdaa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()"
      ],
      "metadata": {
        "id": "lPp9WEXWywdl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models import Doc2Vec\n",
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
        "model_dbow.build_vocab([x for x in tqdm(train_tagged)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibtJ_HxMywhA",
        "outputId": "02f57487-27d3-4fbb-ae15-4ce1a3f151c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1240815.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tagged.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OMvru9Uywrd",
        "outputId": "be742ab2-12e4-4d2f-e33a-500946ad9e59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([TaggedDocument(words=['growing', 'number', 'of', 'covid-19', 'patients', 'whose', 'symptoms', 'were', 'initially', 'mild', 'are', 'now', 'facing', 'mysterious', 'long-term', 'neurological', 'problems', 'url'], tags=[0]),\n",
              "       TaggedDocument(words=['medical', 'eperts', 'advise', 'that', 'symptoms', 'of', 'the', 'novel', 'coronavirus', 'include', 'fever', 'shortness', 'of', 'breath', 'and', 'stinky', 'smelly', 'pits', 'and', 'feet', 'ðŸ˜³ðŸ¤ª'], tags=[0]),\n",
              "       TaggedDocument(words=['drdavidsamadi', 'hubby/i', 'same', 'symptoms', 'november', '2019', 'after', 'weekend', 'trip', 'vegas', 'where', 'bus', 'loads', 'of', 'chinese', 'tourists.1', 'day', 'fever,3', 'days', 'sore', 'throat', 'several', 'weeks', 'of', 'fatigue.he', \"'s\", 'healthy', \"'m\", 'not', 'diabetes', 'hypertension', 'obese', 'respiratory', 'issues', '53.', 'no', 'meds/pneumonia', 'we', 'believe', 'was', 'covid-19'], tags=[1]),\n",
              "       ...,\n",
              "       TaggedDocument(words=['yes', 'but', 'didn', 'recovered', 'with', 'tylenol', 'cough', 'medicine', 'added', 'zinc', 'after', 'my', 'sister', 'niece', 'ended', 'up', 'in', 'hospital', 'for', 'this', 'and', 'given', 'zinc', 'steroids', 'almost', 'immediately', 'the', 'one', 'with', 'prior', 'respiratory', 'problems', 'didn', 'survive'], tags=[1]),\n",
              "       TaggedDocument(words=['hello', 'guys', 'just', 'want', 'to', 'put', 'it', 'out', 'there', 'that', 'please', 'ignore', 'whatsapp', 'messages', 'for', 'covid-19', 'and', 'that', 'it', 'is', 'not', 'serious', 'disease', 'and', 'all', 'lost', 'my', 'father', 'to', 'covid-19', 'last', 'month', 'he', 'was', 'healthy', 'person', 'with', 'no', 'complications', 'of', 'diabetes', 'or', 'bp.he', 'developed', 'fever', 'and', 'the', 'fever'], tags=[1]),\n",
              "       TaggedDocument(words=['``', 'she', 'then', 'again', 'tried', 'to', 'end', 'the', 'call', 'and', 'wanted', 'to', 'schedule', 'the', 'net', 'one', 'the', 'call', 'had', 'only', 'been', 'few', 'minutes', 'so', 'far', 'asked', 'her', 'about', 'my', 'lower', 'body', 'symptoms', 'given', 'the', 'legs', 'mri', 'had', 'shown', 'no', 'signs', 'of', 'damage', 'she', 'just', 'said', 'it', 'was', '``', \"''\", 'post', 'viral', \"''\", \"''\", 'and', 'nothing', 'else', '10/n'], tags=[2])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn import utils\n",
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ksbHOCKy7S3",
        "outputId": "0aa57bd2-7d88-4d84-c16f-48264e52befe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1125740.17it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1781670.39it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1624231.42it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1400418.12it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1176226.47it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1830025.23it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2265428.87it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1136505.72it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2572184.94it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1193314.96it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1268885.07it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1607955.45it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 929300.26it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1320890.36it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1811717.13it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 2592525.35it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1169750.37it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1439648.48it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1376991.61it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1012318.11it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1272110.87it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1189954.45it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1223136.32it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1913157.98it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1450410.16it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1553058.94it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 957880.07it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1158172.57it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1393746.04it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9067/9067 [00:00<00:00, 1036882.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 39.3 s, sys: 2.32 s, total: 41.6 s\n",
            "Wall time: 24.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, regressors"
      ],
      "metadata": {
        "id": "N2SPd5oay9hm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
        "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
        "svcClassifier=SVC(kernel='rbf')\n",
        "svcClassifier.fit(X_train, y_train)\n",
        "y_pred = svcClassifier.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred,average='micro')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBecsm_dy_hA",
        "outputId": "74fb5047-ace4-422d-cf83-1ba001bcc2af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy 0.572\n",
            "Testing F1 score: 0.572\n"
          ]
        }
      ]
    }
  ]
}